# 開発者用のドキュメント の下書き

(注意: 内容がまとまっておらず読みづらい状態です。
ドキュメントは執筆途中であるため、カッコ内にメタ議論 (ドキュメントを書く作業に対する内容) を残しています)

---

このドキュメントは開発者がコードベースを理解することを助けるための情報が書かれています

多数の構成要素が含まれているため、構成要素を列挙し、階層化・グループ分けします。
グループごと、構成要素ごとに詳細に説明します

## 構成要素

- ham-core
    - 言語処理
        - HSPの構文処理
            - トークン、ノード、位置・範囲、ほか
            - トークナイザ
            - パーサー
        - HSPの意味解析(sema)
            - 名前システム (name_system, env, scope)
        - ヘルプソースの構文解析 (help_source)
    - LSPサーバー
        - サーバーのエンジン (src/lsp_server)
        - ドキュメント管理 (docs, uri, location)
        - アナリシス (workspace_analysis, project_analysis, doc_analysis)
        - リクエストハンドラ
            - ドキュメント更新
            - ホバー、入力補完、診断 (diagnosis)、アクション (assists)、ほか
    - ファイルシステム処理
        - common, hsphelp の探索
- ham-lsp
    - LSPサーバーのバイナリ
- vscode-ext: VSCode 拡張機能
    - LSPクライアント (vscode-languageclient)

---

## 羅列

(ドキュメントに含めておくべきと思われる情報を書きます。
適切な順番に並べ替えて文章として上から順に読めるほうがよいでしょう。
序盤で概要やインターフェイスだけ述べて後半で詳細を述べるといった分割があったほうがよいかもしれません)

### プロジェクトの構造に関すること

HAMとは: hsp3-analyzer-mini の頭文字

**Rustのクレートが複数ある理由**:
コンパイルの結果として作るファイルが複数あるからです。
DLLとexeファイルの2つを作ろうとしています。
exeファイルがLSPサーバーの本体です。
DLLはVSCodeの拡張機能では利用されていません。
DLLは、HSPやC言語からHAMの機能の一部を再利用する方法として提供しようかなと思って作ってあります (実用性は不明)。
HAMの実装の重要な部分はすべて ham-core というクレートに含まれています。
ham-core はライブラリクレートであり、DLLとexeの両方から参照されます。両者の共通部分に相当します。

(ham-rewriteは無視してよい)

**hsed3-extとは**: 標準のスクリプトエディタにHAMの機能 (入力補完とか) を持たせるためのツールです。ただし未完成です

**vscode-extとは**: VSCodeの拡張機能であるパッケージです (これの意味が分かるのだろうか?)
LSPクライアントとして動作します
(LSPクライアントの基本的な実装はこのリポジトリに含まれていません。
LSPクライアントの機能は `vscode-languageclient` というパッケージに実装されていて、この拡張機能自身はそれを使っています)

### 言語処理に関すること

**HAMが行う言語処理**:
一般に言語処理系はソースコードを解析する部分と、それを実行可能なプログラムに変換する部分からなります。
HAMはスクリプトの実行を行わないので解析部分しかありません。

**解析部分の内訳**:
解析部分は大きく分けて構文解析と意味解析があります。
構文解析はソースコードを適切なデータ構造 (= 解析木) に変換する工程です。
意味解析は名前解決や型検査などの言語特有の規則にもとづく計算処理を行う工程です。
(このあたりはコンパイラの教科書に詳しく書かれています)

**言語処理の再実装について**:
OpenHSPのソースコードを再利用するのではなく、独自に再実装しています

**プリプロセッサの対応について**:
プリプロセッサ命令への対応は不完全です。
多くのプリプロセッサ命令の構文は解析されますが、HSPのコンパイラとは異なる解釈で処理されます。
(特に、マクロの展開は実装されていません)

**構文解析の一般知識**:
工程順に説明します。はじめに字句解析です。
文法上の基本単位を **トークン** (token) といいます。
(ソースコードは文字列ですが、文字単位で処理するのは不便です。
後続の工程で分けて扱う必要がない部分をグループ化したものがトークンです)
ソースコードを **トークン列** に変換する処理を **トークナイズ** (tokenize) といいます。
(トークナイズの処理は分かりやすいのでコードを読んで理解可能だと思います
文字の種類で分岐して、その文字を含むトークンの範囲を調べ、トークンの種類を決定する、ということを繰り返しています)
(以上が字句解析です。字句解析を行うと後続の工程で文字列処理が不要になり、複雑性が大幅に減ります)

次に構文解析を説明します。
トークン列を **解析木** (parse tree) に変換する処理を **構文解析** (parse) といいます。
(...)

**字句解析の実装方針**:
トークンの種類は `TokenKind` で、トークンの実データを `TokenData` という型で定義しています。
`tokenize_context` は字句解析中に保持するデータを集めた型です。
`tokenize_rules` が字句解析処理です。
(ほかの工程も、処理を行う関数とそれが扱うデータを分離して書くという構造になっています)

**構文解析の実装方針**:
手書きの再帰下降構文解析です。
`PToken` は解析木のリーフノードです。
(字句解析における単一のトークン (TokenData) に加えて、周囲のトリビアをグループ化したものです)
`PTree` が解析木のルートノードです。
(...)

**構文処理による改行の扱い**:
字句解析の時点で改行を対処し、パーサが改行を処理しなくて済むようにする。
字句解析では各行がプリプロセッサ行か通常行かは判別する。
プリプロセッサ行の末尾にある改行はトークンとして出力し、パーサに渡す (EOSトークン)。
プリプロセッサ行はバックスラッシュによって行継続されることがあるが、行継続記号と後続する改行をまとめて1個のトークンとして出力し、これはパーサに渡さない (Blankトークン)
(パーサは行継続の存在を無視できる)。
空行は単に無視してよい。
プリプロセッサ行ではない、通常行の末尾にEOSトークンを出力する。
EOSトークンは、他の言語のセミコロンのように文を終止させる役割を果たす。
パーサは単にEOSが見つかるたびに文を区切るという挙動をすればよい (セミコロンがある言語と同様になる)。
また、複数行に渡る字句は単一の字句としてパーサに渡されるため、内部の改行は無視できる。
改行の存在は行番号を変化させるが、字句解析中にトークンごとに位置を計算しておけば、パーサは無視できる。

## 資料

(よい資料があったら教えてもらえると助かります)

- 書籍:
    - 実践コンパイラ構成法
